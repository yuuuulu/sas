---
title: "EM Algorithm"
format: html
editor: visual
---

# Mixture Problems

## Eg of exponential function

### Background

$$Y_1 \sim \text{Exp} (\frac{1}{\theta_1})$$
$$Y_2 \sim \text{Exp} (\frac{1}{\theta_2})$$

$$W \sim \text{Bernoulli} $$

W is independent of $Y_1$ and $Y_2$ with probability of success $\epsilon = P(W=1)$

- Observed: 

$X$ = $WY_1+(1-W)Y_2$

$\theta '=(\theta_1,\theta_2,\epsilon)$  -- parameters here

pdf of x: $f(x) = \epsilon f_1(x) + (1-\epsilon) f_2(x)$

where $f_1(x) = \frac{1}{\theta_1} e^{-\frac{x}{\theta_1}}$, $f_2(x) = \frac{1}{\theta_2} e^{-\frac{x}{\theta_2}}$ 

Suppose we observed a random sample $X' = (X_1, X_2, \ldots, X_n)$ from this mixture distribution with pdf f(x).

Then 
 $$ l(\theta | X) = \sum_{i=1}^n \log \left[ \epsilon f_1(X_i) + (1 - \epsilon) f_2(X_i) \right]$$

- Unobserved

$$W_i = \begin{cases} 
0 & \text{if } X_i \sim \text{pdf } f_2(x) \\
1 & \text{if } X_i \sim \text{pdf } f_1(x)
\end{cases} $$


$W_1, \ldots, W_n \sim \text{Bernoulli r.v. with successuful probability of } \epsilon$

### Solution

$$\begin{align*}
l^c(\theta | x, w) &= \sum_{W_i=0} \log f_2(X_i) + \sum_{W_i=1} \log f_1(X_i) \\
&= \sum_{i=1}^n \left[ w_i \log f_1(X_i) + (1 - w_i) \log f_2(X_i) \right]
\end{align*}$$



\begin{align*}
Q(\theta | \theta_0, X) &= \mathbb{E}_{\theta_0} \left[ \sum_{i=1}^n \left[ w_i \log f_1(x_i) + (1 - w_i) \log f_2(x_i) \right] \right] \\

\end{align*}


$$\gamma_i = \frac{\hat \epsilon f_1(x_i)}{\hat \epsilon f_1(x_i) + (1 - \hat \epsilon) f_2(x_i)}$$

We get

$$\begin{align*}
Q(\theta | \theta_0, X) &=\sum_{i=1}^n \left[ \gamma_i \log f_1(x_i) + (1 - \gamma_i) \log f_2(x_i) \right]  \\
\end{align*}
$$

from

$$ \frac{\partial Q}{\partial \theta_1} = \sum_{i=1}^n \gamma_{i} \left( \theta_1 - x_i \right)=0$$

we get $\theta_1 = \frac{\sum_{i=1}^n \gamma_{i} x_i} {\sum_{i=1}^n \gamma_{i}}$


Similarly, $\theta_2 = \frac{\sum_{i=1}^n (1-\gamma_{i} )x_i} {\sum_{i=1}^n (1-\gamma_{i} )}$

$\hat \epsilon= \frac{\sum_{i=1}^n \gamma_{i}} {n}$

